# K-Means Clustering: Conceptual Overview



this is code for a manually implemented k-means algorthm. do not have to use if do not want to, but could be useful for explaining the algorithm? not sure tbh j wanted to give the option
```{python}
class kMeans():
    def __init__(self, K, X):
        self.K = K
        self.centroids = X[np.random.choice(X.shape[0], size=K, replace=False)]
        self.assignments = np.zeros(X.shape[0], dtype=int)
        self.X = X

    def update_centroids(self, closest_centroids):
        X_grouped = [self.X[closest_centroids == idx] for idx in range(len(self.centroids))]
        self.centroids = [X.mean(axis=0) for X in X_grouped]
        
    def find_cluster(self):
        while True:
            distances = np.array([np.linalg.norm(self.X - centroid, axis=1) for centroid in self.centroids]).T
            closest_centroids = np.argmin(distances, axis=1)

            if np.array_equal(closest_centroids,self.assignments):
                break
            else:
                self.update_centroids(closest_centroids)
                self.assignments = closest_centroids

        return self.assignments
```

# Practical Example
Before we begin, we will start by loading in the necessary libraries. 

```{python}
import sklearn as sk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns

from sklearn.cluster import KMeans
```

## Code Implementation
Process: First, we concatentated the white wine and red wine datasets. We will then do data preprocessing such as potentially dropping some of the white wine samples so that the data is more balanced. This is followed by Exploratory Data Analysis to understand the variables using histograms of variable distribution and heatmaps to understand variable correlation. We will then normalize and standardize the variables with strongly skewed distributions and outliers using log transformation and other methods, before applying the k-means clustering algorithm.

Goal: To predict the color of wine based on a variety of factors, such as pH, alcohol level, and more.


## Dataset Description:
We will show an example of running the k-means clustering algorithm on the Wine Quality from UC Irvine. This dataset contains wine quality data, with features such as `citric_acid`, `density`, `pH`, and more. The response variable of this dataset is `wine_quality`, which is rated on a scale from 0-10. This dataset is split into 2 sub-datasets, one regarding red wine and one regarding white wine. Since k-means clustering is best suited to classification tasks, we will attempt to group the data into red wine and white wine rather than predicting `wine_quality`. 

We will start by creating the response variable `color` for each dataset and then merging the two datasets into one.

```{python}
red = pd.read_csv('data/winequality-red.csv', sep=';', header=0)  
white = pd.read_csv('data/winequality-white.csv', sep=';', header=0) 

red['color'] = 'red'
white['color'] = 'white'

wine_data = pd.concat([red, white], axis=0)
print(len(wine_data))
print(pd.value_counts(wine_data['color']))
```

 We now have one dataset `wine_data` that has 6947 observations total, with 4898 white wine observations and 1599 red wine observations. This is quite unbalanced, so when we create our training dataset, we will have to stratify our sampling to ensure an even proportion of red and white wine. 
 ## FIX ME is this fine? shoudl we drop some of the white wine samples?


## Exploratory Data Analysis
Prior to applying the algorithm to our data, we should first explore the dataset to get an idea of the structure of this dataset. 

First, let us take a look at the correlations between features.
```{python}
correlation_matrix = wine_data.drop(columns=['color']).corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title('Correlation Matrix of Wine Quality Dataset')
plt.show()
```

There are a few relationships to note. The highest correlated variables are `total sulfur dioxide` and `free sulfur dioxide`, which exhibit a positive correlation. This makes sense as total sulfur dioxide is usually calculated the sum of free sulfur dioxide and bound sulfure dioxide. This could support an argument for dropping one of these predictors from our analysis, but we do not have any measure for bound sulfur dioxide and do not have additional information about the calculation of these variables readily avaible, so it could be beneficial to keep both predictors. ## FIX ME decide if we want to drop or not. Another relationship of note is the negative correlation between `density` and `alcohol`. This is another expected relation as higher ABV liquids are generally less dense. ## FIX ME add more here?

### HISTOGRAMS
```{python}
print(wine_data.head())
```

```{python}
wine_data.hist(bins=20, figsize=(15, 10))
plt.show()
```

For each numerical variable, we generated histograms to visualize the distribution and analyze the spread of data. This analysis will help us identify patterns in the data, such as symmetry, skewness, and outliers, which will help us determine what data preprocessing should be done in order to create the best k-means algorithm. For example, the graphs show us that many of the variables, such as `volatile acidity`, `residual sugar`, `chlorides`, and `sulphates`, are heavily right-skewed, indicating that we might need to normalize or scale these features before applying the algorithm, potentially through log-transformation or other methods. Additionally, variables like `residual sugar`, `total sulfur dioxide`, and `alcohol` have long right tails, which indicate the presence of significant outliers. Because of these outliers, we could run into issues with the clustering results if we don't address them. Lastly, `pH`, `quality`, and `density` all have more symmetric, bell-shaped distributions than the other variables, which are indicators that these would be good features to include in k-means clustering. 
## Potentially switch order of feature histograms and correlation matrix

# References
Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Wine Quality [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C56S3T.
# K-Means Clustering: Conceptual Overview
K-means clustering is a form of unsupervised learning that divides ***n*** observations into ***k*** groups, called clusters. 

The split of data into groups is unknown, and the data points fall into a certain cluster based on the mathematical distance measurement from the center of the cluster. In most instances, the mathematical distance measurement is Euclidean, with the mean or median of the cluster being the determining factor of whether or not a data point belongs.

Clusters are then randomly assigned a centroid, or center, in the space. Each data point is then assigned to one of the clusters. After assigning each point to one of the clusters, new centroids are assigned. This is repeated until a "good" cluster is found.

An apt division minimizes the variation within each cluster with maximized variation between each cluster. This means we want data points within a cluster to be similar to each other, but we also want the data points of each cluster to be distinctly different from other clusters, so that clusters formed by k-means clustering are "compact and isolated".

## K-means algorithm
The process for K-means clustering is as follows:\
1. initialize K: set k equal to the number of clusters desired\
2. Assign centroids:

- Expectation step: assign data points to closest centroid based on predetermined measurement of distance
- Maximization step: reassigns center of cluster based on mean of data points in cluster

Expectation and maximization steps repeat until we reach convergence.

Here, we have code for the manual implementation of k-means clustering, showing each step of the k-means clustering algorithm in Python.

We start by initializing K, the number of clusters, and selecting K random data points from the dataset as the initial centroids. We also intialize a vector of assignmnets that is the samelength as the dataset to store the cluster assignment for each data point. The dataset itself is stored as X. To find clusters, we call `find_cluster`, which enters a loop that runs until the algorithm converges. At each iteration, the distances from every data point to each centroid are calculated, resulting in a matrix where rows represent data points and columns represent centroids. Each data point is then assigned to the closest centroid. Convergence is checked by comparing the current assignments with those from the previous iterationâ€”if they are the same, the algorithm stops and returns the cluster assignments. If not, we call `update_centroids`. In this method, the centroids are updated by grouping data points by their current assignments and recalculating the mean position of each cluster, which becomes the new centroid. This process repeats until the cluster assignments match those of the previous iteration.

```{python}
class kMeans():
    def __init__(self, K, X):
        self.K = K
        self.centroids = X[np.random.choice(X.shape[0], size=K, replace=False)]
        self.assignments = np.zeros(X.shape[0], dtype=int)
        self.X = X

    def update_centroids(self, closest_centroids):
        '''This method will update the stored centroids according to the new clusters '''
        X_grouped = [self.X[closest_centroids == idx] for idx in range(len(self.centroids))]
        self.centroids = [X.mean(axis=0) for X in X_grouped]
        
    def find_cluster(self):
        ''' This method will iteratively search for clusters'''
        while True:
            distances = np.array([np.linalg.norm(self.X - centroid, axis=1) for centroid in self.centroids]).T
            closest_centroids = np.argmin(distances, axis=1)

            if np.array_equal(closest_centroids,self.assignments):
                break
            else:
                self.update_centroids(closest_centroids)
                self.assignments = closest_centroids

        return self.assignments
```

# Practical Example
Before we begin, we will start by loading in the necessary libraries. 
```{python}
import sklearn as sk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns

from sklearn.cluster import KMeans
```


## Dataset Description:
We will show an example of running the k-means clustering algorithm on the Wine Quality from UC Irvine. This dataset contains wine quality data, with features such as `citric_acid`, `density`, `pH`, and more. The response variable of this dataset is `wine_quality`, which is rated on a scale from 0-10. This dataset is split into 2 sub-datasets, one regarding red wine and one regarding white wine. Since k-means clustering is best suited to classification tasks, we will attempt to group the data into red wine and white wine rather than predicting `wine_quality`. 

We will start by creating the response variable `color` for each dataset and then merging the two datasets into one.

```{python}
red = pd.read_csv('data/winequality-red.csv', sep=';', header=0)  
white = pd.read_csv('data/winequality-white.csv', sep=';', header=0) 

red['color'] = 'red'
white['color'] = 'white'

wine_data = pd.concat([red, white], axis=0)
print(len(wine_data))
print(pd.value_counts(wine_data['color']))
```

We now have one dataset `wine_data` that has 6947 observations total, with 4898 white wine observations and 1599 red wine observations. This is quite unbalanced, so when we create our training dataset, we will have to stratify our sampling to ensure an even proportion of red and white wine. 
 ## FIX ME is this fine? shoudl we drop some of the white wine samples?


## Code Implementation
**Process:** First, we concatentate the white wine and red wine datasets. Then, we embark on data preprocessing, such as potentially dropping some of the white wine samples so that the data is more balanced. This is followed by Exploratory Data Analysis to understand the variables using histograms of variable distribution and heatmaps to understand variable correlation. We then normalize and standardize the variables with strongly skewed distributions and outliers using log transformation and other methods, before applying the k-means clustering algorithm.

**Goal:** To predict the color of wine based on a variety of factors, such as pH, alcohol level, and more.

**Outcomes:** \
- Cluster Assignments: Each wine sample will be assigned to a cluster based on its properties \
- Information from Centroids: The centroids of each cluster may provide information on the differences between red and white wines (i.e. one cluster may have higher density, which is indicative of red wines) \

**Impact of K-means Clustering:** Relationships observed during EDA can be reflected in distinct centroids and important features are highlighted through clustering

## Exploratory Data Analysis
Prior to applying the algorithm to our data, we should first explore the dataset to get an idea of the structure of this dataset. 

First, let us take a look at the correlations between features.
```{python}
correlation_matrix = wine_data.drop(columns=['color']).corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title('Correlation Matrix of Wine Quality Dataset')
plt.show()
```

There are a few relationships to note. The highest correlated variables are `total sulfur dioxide` and `free sulfur dioxide`, which exhibit a positive correlation. This makes sense as total sulfur dioxide is usually calculated the sum of free sulfur dioxide and bound sulfur dioxide. This could support an argument for dropping one of these predictors from our analysis, since they are directly correlated, leaving both of these variables in could lead to multicollinearity and issues when we apply our algorithm. We can therefore drop `free sulfur dioxide`. 

```{python}
wine_data = wine_data.drop(columns=['free sulfur dioxide'])
```

 Another relationship of note is the negative correlation between `density` and `alcohol`. This is another expected relation as higher ABV liquids are generally less dense. ## FIX ME add more here?

### HISTOGRAMS

```{python}
wine_data.hist(bins=20, figsize=(15, 10))
plt.show()
```

For each numerical variable, we generated histograms to visualize the distribution and analyze the spread of data. This analysis will help us identify patterns in the data, such as symmetry, skewness, and outliers, which will help us determine what data preprocessing should be done in order to create the best k-means algorithm. For example, the graphs show us that many of the variables, such as `fixed acidity`, `volatile acidity`, `residual sugar`, `chlorides`, and `sulphates`, are heavily right-skewed, indicating that we might need to normalize or scale these features before applying the algorithm, potentially through log-transformation or other methods. Additionally, variables like `residual sugar`, `total sulfur dioxide`, and `alcohol` have long right tails, which indicate the presence of significant outliers. Because of these outliers, we could run into issues with the clustering results if we don't address them. Lastly, `pH`, `quality`, and `density` all have more symmetric, bell-shaped distributions than the other variables, which are indicators that these would be good features to include in k-means clustering. 

## Data Preprocessing
Now that we have conducted our EDA, we know how to move forward with our data preprocessing. We will first be removing outliers, as the graphs show that there are significant and extreme outliers in the distributions of some of the variables. We will then be log transforming and square root transforming the remaining variables to get rid of any right-skewedness the variables may still have. 

### Outlier Removal
Outliers are an issue when it comes to applying a clustering algorithm like k-means. Their presence can distort the centroid of the cluster, pulling them away from most of the datapoints. In order to address this, we should remove the extreme outliers before transformation so we don't accidentally amplify them. 

```{python}
def remove_extreme_outliers_iqr(data, column, multiplier=3):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]

# Remove only extreme outliers
columns_to_clean = ['fixed acidity', 'volatile acidity', 'chlorides', 'sulphates']
for col in columns_to_clean:
    wine_data = remove_extreme_outliers_iqr(wine_data, col, multiplier=3)
```

### Scaling and Normalizing variables
We have determined through manual inspection of the graphs that some variables definitely need transformation and normalization. To determine exactly which ones need this, we can run the skew function, which calculates the Fisher-Pearson coefficient of skewness. 
```{python}
numerical_columns = wine_data.select_dtypes(include=['number'])

skewness = numerical_columns.skew()

print(skewness)
```

This analysis confirms our initial thought that `chlorides`, `fixed acidity`, `volatile acidity`, `residual sugar`, `sulphates`, and `alcohol` are the most heavily skewed, even after outlier removal. We will be using log transformation on most variables and square root transformation on the other 2. 

We will first be dealing with the most heavily skewed variables. We will be using log transformation to normalize these variables. 

```{python}
# Log transformation for heavy skew
log_transform_vars = ['residual sugar', 'fixed acidity', 'volatile acidity', 'chlorides']
for col in log_transform_vars:
    wine_data[col] = np.log1p(wine_data[col])
```

We will then be dealing with the moderately skewed variables, using square root transformation. 

```{python}
# Square root transformation for moderate skew
sqrt_transform_vars = ['sulphates', 'alcohol']
for col in sqrt_transform_vars:
    wine_data[col] = np.sqrt(wine_data[col])
```

We will be leaving all variables with a skewness less than 0.5 unchanged, as those are close to symmetric distributions. We can recheck the skewness of the variables after the transformation to see if they are more normalized. 

```{python}
skewness = numerical_columns.skew()

# Print skewness values
print(skewness)
```

We have now run transformation on all of the skewed variables. Though `volatile acidity`, `fixed acidity`, and `chlorides` are still moderately to heavily skewed, they have improved from previously, and it is important to not over-transform the variables because we can then lose valuable information about the variables and distort the data. We are now ready to apply the k-means algorithm. 

# References
Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Wine Quality [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C56S3T.\
IBM. (n.d.). K-means clustering. Retrieved December 3, 2024, from https://www.ibm.com/topics/k-means-clustering
GeeksforGeeks. (2024). K means Clustering - Introduction. Retrieved December 3, 2024, from https://www.geeksforgeeks.org/k-means-clustering-introduction/
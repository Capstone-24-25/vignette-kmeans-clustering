---
title: "draft"
format: html
editor: visual
---

## Information for README

1.  One-sentence description:

    > *Vignette on implementing k-means clustering using \_\_\_\_ data; created as a class project for PSTAT197A in Fall 2024.*

2.  Contributors: Sophia Mirrashidi, Leena Anqud, Nazhah Mir, Hannah Kim

3.  Vignette abstract: a brief description in a few sentences of your vignette topic, example data, and outcomes.

4.  Repository contents: scripts (draft), data (), README.md, vignette.qmd

5.  Reference list: 2 or more references to learn more about your topic.

A typical README file would also contain instructions on use and instructions on contributing to the repository.

## Information of k-means clustering:

**Source 1**: PSTAT131 slides

Clustering: dividing data into unknown groups

-   k-means clustering: dividing observations into specified number of groups where k equals number of groups

    -   minimize the variation within each cluster for a good fit

Use cases: pre-processing,

**Source 2**: <https://www.ibm.com/topics/k-means-clustering>

Partitions in k-means clustering determined by distance between centroids

-   Centroid: center of cluster, mean/median of all points within cluster

-   Data points fall into a certain cluster based on the mathematical distance measurement from the center

<!-- -->

-   k-means form of unsupervised learning, so split into groups is unlabelled/unclassified/unknown

Large k: larger number of clusters, smaller amount of data points in each cluster, more detailed information about cluster

Smaller k: less number of clusters, greater amount of data points in each cluster, less details regarding data per cluster

K-means clustering process:

1.  initialize K: set k equal to the number of clusters desired
2.  Assign centroids:
    1.  Expectation step: assign data points to closest centroid based on predetermined measurement of distance
    2.  Maximization step: reassigns center of cluster based on mean of data points in cluster
    3.  Expectation and maximization steps repeat until we reach convergence

Evaluation metrics:

1.  Minimize intracluster distance
2.  Maximize intercluster distance

This means we want data points within a cluster to be similar to each other, but we also want the data points of each cluster to be distinctly different from other clusters. A good k-means clustering algorithm has clusters that are "compact and isolated" from each other.

Use Cases: market segmentation, document clustering, image segmentation, image compression, recommendation engines

Benefits: works well with machine learning data that is numeric with few dimensions and can be easily proportioned, enhances performance of machine learning tasks, simple, fast, scalable

Drawbacks: sensitive to outliers, dependence on input parameters, difficulty working with high dimension/variation data

**Source 3:**

## Outline of Code Implementation

Potential dataset: wine_data

Process: First, we concatentated the white wine and red wine datasets. We will then do data preprocessing such as potentially dropping some of the white wine samples so that the data is more balanced. This is followed by Exploratory Data Analysis to understand the variables using histograms of variable distribution and heatmaps to understand variable correlation. We will then normalize and standardize the variables with strongly skewed distributions using log transformation and other methods, before applying the k-means clustering algorithm. 

Goal: To predict the color of wine based on a variety of factors. 

Outcome:

Impact of k-means clustering:
